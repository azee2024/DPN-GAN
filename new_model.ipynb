{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb76df8b-a6ff-4c87-9686-15a9f9f4310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbf96be-cd67-4d83-a15d-2216b57ce0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef634e86-9fee-4b41-bf5a-c50d1bcb7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMNISTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.file_list = os.listdir(self.data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = os.path.join(self.data_path, self.file_list[idx])\n",
    "        data = np.load(file_name)\n",
    "        \n",
    "        # print(self.file_list[idx])\n",
    "        label, subfolder_index, data_index = self.file_list[idx].split(\"_\")\n",
    "        \n",
    "        # Extracting data\n",
    "        metadata = data[\"metadata\"]\n",
    "        audio = data[\"audio\"]\n",
    "        mel_spec = data[\"mel_spec\"]\n",
    "        mel_spec_db = data[\"mel_spec_db\"]\n",
    "\n",
    "        # Assuming your data is a numpy array, you can convert it to a PyTorch tensor\n",
    "        tensor_audio = torch.from_numpy(audio).reshape(-1, 1)\n",
    "        tensor_meta = torch.tensor(metadata).reshape(-1, 1)\n",
    "        tensor_label = torch.nn.functional.one_hot(torch.tensor(int(label)), num_classes=10).reshape(-1, 1)\n",
    "        tensor_mel_spec = torch.tensor(mel_spec)\n",
    "        tensor_mel_spec_db = torch.tensor(mel_spec_db)\n",
    "        return tensor_audio, tensor_meta, tensor_label, tensor_mel_spec, tensor_mel_spec_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a4e9a9e-a5a8-48e4-b1f2-9a162c1cc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioMNISTDataset(data_path=\"./AudioMNIST/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b639f4ae-668c-4b19-a0c7-31e87cee081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f5f7d5-66de-4090-a6e4-7c723a6151ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0780c91-d117-485a-bc6c-8276c45964fe",
   "metadata": {},
   "source": [
    "### **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d63eaef-d4e5-44bd-9485-f17f7812ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, metadata_dim, audio_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.audio_dim = audio_dim\n",
    "\n",
    "        # Generator layers\n",
    "        self.fc1 = nn.Linear(noise_dim + metadata_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, audio_dim)\n",
    "\n",
    "    def forward(self, noise, metadata):\n",
    "        # Concatenate noise and metadata\n",
    "        x = torch.cat((noise, metadata), dim=1)\n",
    "        # Pass through generator layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))  # Use tanh activation for audio generation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c452ed7-f963-4355-9e40-754c0940f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, audio_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "\n",
    "        # Discriminator layers for real/fake classification\n",
    "        self.fc1 = nn.Linear(audio_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "        # Discriminator layers for classifying the generated number\n",
    "        self.fc3 = nn.Linear(audio_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)  # Output is 10 classes (0-9)\n",
    "\n",
    "    def forward(self, audio):\n",
    "        # Discriminator for real/fake classification\n",
    "        # print(audio.shape)\n",
    "        audio = torch.transpose(audio, 1, 2)\n",
    "        x1 = torch.relu(self.fc1(audio))\n",
    "        validity = torch.sigmoid(self.fc2(x1))\n",
    "\n",
    "        # Discriminator for classifying the generated number\n",
    "        x2 = torch.relu(self.fc3(audio))\n",
    "        generated_number = torch.softmax(self.fc4(x2), dim=1)\n",
    "\n",
    "        return validity, generated_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7e0efd83-aa93-47c7-8d52-7cdcc8c01c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function for GAN\n",
    "def train_gan(generator, discriminator, data_loader, num_epochs, noise_dim, metadata_dim, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_audio, meta, labels, mel_spec, mel_spec_db) in enumerate(data_loader):\n",
    "            # Move data to device\n",
    "            real_audio = real_audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Generate random noise and metadata\n",
    "            noise = torch.randn(real_audio.size(0), noise_dim, device=device)\n",
    "            metadata = torch.randint(0, 10, (real_audio.size(0), metadata_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "            # Generate fake audio samples\n",
    "            generated_audio = generator(noise, metadata)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real audio\n",
    "            real_labels = torch.ones(real_audio.size(0), 1, device=device)\n",
    "            real_validity, real_generated_number = discriminator(real_audio)\n",
    "            \n",
    "            # Squeeze the tensor\n",
    "            real_validity = torch.squeeze(real_validity, dim=-1)\n",
    "            # print(real_validity.shape)\n",
    "            \n",
    "            d_loss_real = adversarial_loss(real_validity, real_labels)\n",
    "            # print(real_generated_number.shape)\n",
    "            # print(labels.shape)\n",
    "            real_generated_number = torch.transpose(real_generated_number, 1, 2)\n",
    "            d_loss_real_classification = classification_loss(real_generated_number, labels.float())\n",
    "\n",
    "            # Fake audio\n",
    "            fake_labels = torch.zeros(real_audio.size(0), 1, device=device)\n",
    "\n",
    "            generated_audio = torch.unsqueeze(generated_audio, dim=-1)\n",
    "            fake_validity, fake_generated_number = discriminator(generated_audio.detach())\n",
    "            \n",
    "            # Squeeze the tensor\n",
    "            fake_validity = torch.squeeze(fake_validity, dim=-1)\n",
    "            \n",
    "            d_loss_fake = adversarial_loss(fake_validity, fake_labels)\n",
    "            \n",
    "            fake_generated_number = torch.transpose(fake_generated_number, 1, 2)\n",
    "            d_loss_fake_classification = classification_loss(fake_generated_number, labels.float())\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake + d_loss_real_classification + d_loss_fake_classification\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            validity, generated_number = discriminator(generated_audio)\n",
    "            validity = torch.squeeze(validity, dim=-1)\n",
    "            \n",
    "            g_loss = adversarial_loss(validity, real_labels)\n",
    "            \n",
    "            generated_number = torch.transpose(generated_number, 1, 2)\n",
    "            # g_loss_classification = classification_loss(generated_number, labels.float())\n",
    "\n",
    "            # g_loss_total = g_loss + g_loss_classification\n",
    "            g_loss_total = g_loss\n",
    "            g_loss_total.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Print training progress\n",
    "            if i % 100 == 0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %.4f] [G loss: %.4f]\" # [D classification loss: %.4f] [G classification loss: %.4f]\"\n",
    "                    % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item()) #, d_loss_real_classification.item(), g_loss_classification.item())\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d0de5323-951b-4a7b-99de-2d158793f18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 8  # Size of input noise vector\n",
    "noise_dim = input_size\n",
    "metadata_size = 5  # Size of metadata array\n",
    "metadata_dim = metadata_size\n",
    "output_size = dataset[0][0].shape[0]  # Size of output audio vector\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccd3013d-3677-4ad0-ad37-142e103e0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = Generator(input_size, metadata_size, output_size).to(device)\n",
    "discriminator = Discriminator(output_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7187b9b5-7397-42e3-97d6-95f7234e4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss()  # Binary cross-entropy loss for real/fake classification\n",
    "classification_loss = nn.CrossEntropyLoss()  # Cross-entropy loss for classifying the generated number\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bf731390-10df-4fa1-9c82-35cbd4ffe96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2] [Batch 0/938] [D loss: 5.9786] [G loss: 9.6873]\n",
      "[Epoch 0/2] [Batch 100/938] [D loss: 5.2992] [G loss: 7.6101]\n",
      "[Epoch 0/2] [Batch 200/938] [D loss: 7.1094] [G loss: 28.0421]\n",
      "[Epoch 0/2] [Batch 300/938] [D loss: 5.2196] [G loss: 25.0088]\n",
      "[Epoch 0/2] [Batch 400/938] [D loss: 5.1895] [G loss: 6.8425]\n",
      "[Epoch 0/2] [Batch 500/938] [D loss: 5.1622] [G loss: 5.8792]\n",
      "[Epoch 0/2] [Batch 600/938] [D loss: 5.1313] [G loss: 5.1715]\n",
      "[Epoch 0/2] [Batch 700/938] [D loss: 5.0916] [G loss: 5.2993]\n",
      "[Epoch 0/2] [Batch 800/938] [D loss: 5.0500] [G loss: 5.3134]\n",
      "[Epoch 0/2] [Batch 900/938] [D loss: 5.0136] [G loss: 5.4749]\n",
      "[Epoch 1/2] [Batch 0/938] [D loss: 4.9912] [G loss: 6.1473]\n",
      "[Epoch 1/2] [Batch 100/938] [D loss: 4.9618] [G loss: 5.3526]\n",
      "[Epoch 1/2] [Batch 200/938] [D loss: 4.9384] [G loss: 7.8755]\n",
      "[Epoch 1/2] [Batch 300/938] [D loss: 4.9436] [G loss: 25.8652]\n",
      "[Epoch 1/2] [Batch 400/938] [D loss: 4.9574] [G loss: 7.7471]\n",
      "[Epoch 1/2] [Batch 500/938] [D loss: 4.9199] [G loss: 7.1869]\n",
      "[Epoch 1/2] [Batch 600/938] [D loss: 4.8931] [G loss: 6.4984]\n",
      "[Epoch 1/2] [Batch 700/938] [D loss: 4.8680] [G loss: 6.0488]\n",
      "[Epoch 1/2] [Batch 800/938] [D loss: 4.8469] [G loss: 5.2637]\n",
      "[Epoch 1/2] [Batch 900/938] [D loss: 4.8385] [G loss: 4.8439]\n"
     ]
    }
   ],
   "source": [
    "train_gan(generator, discriminator, dataloader, num_epochs, noise_dim, metadata_dim, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9f618-9003-467d-bd52-b913e5a6200f",
   "metadata": {},
   "source": [
    "### **Upgrade1: Dynamic Time Warping Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f97ef6d7-7b81-464a-b014-3228c64f9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ced8de6d-30a2-448a-8674-f17fff216c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicTimeWarpingLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DynamicTimeWarpingLoss, self).__init__()\n",
    "\n",
    "    def forward(self, generated_audio, real_audio):\n",
    "        # Convert torch tensors to numpy arrays\n",
    "        generated_audio_np = generated_audio.detach().cpu().numpy().squeeze()\n",
    "        real_audio_np = real_audio.detach().cpu().numpy().squeeze()\n",
    "        \n",
    "        # Compute DTW distance\n",
    "        dtw_distance, _ = fastdtw(generated_audio_np, real_audio_np)\n",
    "        \n",
    "        # Convert distance to torch tensor\n",
    "        dtw_distance_tensor = torch.tensor(dtw_distance, dtype=torch.float32, device=generated_audio.device)\n",
    "        \n",
    "        return dtw_distance_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c22da22a-17e8-4db0-a783-82210b736882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTW Loss: 85.52569580078125\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "loss_fn = DynamicTimeWarpingLoss()\n",
    "\n",
    "# Generate example sequences x and y\n",
    "x = torch.randn(100)  # Example sequence 1\n",
    "y = torch.randn(150)  # Example sequence 2\n",
    "\n",
    "# Compute DTW loss\n",
    "dtw_loss = loss_fn(x, y)\n",
    "print(\"DTW Loss:\", dtw_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "777677b5-3504-49ff-b8e5-f150626989bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class GeneratorDTW(nn.Module):\n",
    "    def __init__(self, noise_dim, metadata_dim, audio_dim):\n",
    "        super(GeneratorDTW, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.metadata_dim = metadata_dim\n",
    "        self.audio_dim = audio_dim\n",
    "\n",
    "        # Generator layers\n",
    "        self.fc1 = nn.Linear(noise_dim + metadata_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, audio_dim)\n",
    "\n",
    "    def forward(self, noise, metadata):\n",
    "        # Concatenate noise and metadata\n",
    "        x = torch.cat((noise, metadata), dim=1)\n",
    "        # Pass through generator layers\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))  # Use tanh activation for audio generation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "45b2a94c-1d88-4a53-8544-599a9647e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class DiscriminatorDTW(nn.Module):\n",
    "    def __init__(self, audio_dim):\n",
    "        super(DiscriminatorDTW, self).__init__()\n",
    "        self.audio_dim = audio_dim\n",
    "\n",
    "        # Discriminator layers for real/fake classification\n",
    "        self.fc1 = nn.Linear(audio_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "        # Discriminator layers for classifying the generated number\n",
    "        self.fc3 = nn.Linear(audio_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)  # Output is 10 classes (0-9)\n",
    "\n",
    "    def forward(self, audio):\n",
    "        # Discriminator for real/fake classification\n",
    "        # print(audio.shape)\n",
    "        audio = torch.transpose(audio, 1, 2)\n",
    "        x1 = torch.relu(self.fc1(audio))\n",
    "        validity = torch.sigmoid(self.fc2(x1))\n",
    "\n",
    "        # Discriminator for classifying the generated number\n",
    "        x2 = torch.relu(self.fc3(audio))\n",
    "        generated_number = torch.softmax(self.fc4(x2), dim=1)\n",
    "\n",
    "        return validity, generated_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e0ac5f30-18ba-4db3-8868-5aff7b9dae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function for GAN\n",
    "def train_gan_dtw(generator, discriminator, data_loader, num_epochs, noise_dim, metadata_dim, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_audio, meta, labels, mel_spec, mel_spec_db) in enumerate(data_loader):\n",
    "            # Move data to device\n",
    "            real_audio = real_audio.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Generate random noise and metadata\n",
    "            noise = torch.randn(real_audio.size(0), noise_dim, device=device)\n",
    "            metadata = torch.randint(0, 10, (real_audio.size(0), metadata_dim), dtype=torch.float32, device=device)\n",
    "\n",
    "            # Generate fake audio samples\n",
    "            generated_audio = generator(noise, metadata)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real audio\n",
    "            real_labels = torch.ones(real_audio.size(0), 1, device=device)\n",
    "            real_validity, real_generated_number = discriminator(real_audio)\n",
    "            \n",
    "            # Squeeze the tensor\n",
    "            real_validity = torch.squeeze(real_validity, dim=-1)\n",
    "            # print(real_validity.shape)\n",
    "            \n",
    "            d_loss_real = adversarial_loss(real_validity, real_labels)\n",
    "            # print(real_generated_number.shape)\n",
    "            # print(labels.shape)\n",
    "            real_generated_number = torch.transpose(real_generated_number, 1, 2)\n",
    "            d_loss_real_classification = classification_loss(real_generated_number, labels.float())\n",
    "\n",
    "            # Fake audio\n",
    "            fake_labels = torch.zeros(real_audio.size(0), 1, device=device)\n",
    "\n",
    "            generated_audio = torch.unsqueeze(generated_audio, dim=-1)\n",
    "            fake_validity, fake_generated_number = discriminator(generated_audio.detach())\n",
    "            \n",
    "            # Squeeze the tensor\n",
    "            fake_validity = torch.squeeze(fake_validity, dim=-1)\n",
    "            \n",
    "            d_loss_fake = adversarial_loss(fake_validity, fake_labels)\n",
    "            \n",
    "            fake_generated_number = torch.transpose(fake_generated_number, 1, 2)\n",
    "            d_loss_fake_classification = classification_loss(fake_generated_number, labels.float())\n",
    "\n",
    "            d_loss = d_loss_real + d_loss_fake + d_loss_real_classification + d_loss_fake_classification\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            validity, generated_number = discriminator(generated_audio)\n",
    "            validity = torch.squeeze(validity, dim=-1)\n",
    "            \n",
    "            # print(generated_audio.shape)\n",
    "            # print(real_audio.shape)\n",
    "            \n",
    "            g_loss = g_adversarial_loss(generated_audio, real_audio)\n",
    "            \n",
    "            generated_number = torch.transpose(generated_number, 1, 2)\n",
    "            # g_loss_classification = classification_loss(generated_number, labels.float())\n",
    "\n",
    "            # g_loss_total = g_loss + g_loss_classification\n",
    "            g_loss_total = g_loss\n",
    "            g_loss_total.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Print training progress\n",
    "            if i % 100 == 0:\n",
    "                print(\n",
    "                    \"[Epoch %d/%d] [Batch %d/%d] [D loss: %.4f] [G loss: %.4f]\" # [D classification loss: %.4f] [G classification loss: %.4f]\"\n",
    "                    % (epoch, num_epochs, i, len(data_loader), d_loss.item(), g_loss.item()) #, d_loss_real_classification.item(), g_loss_classification.item())\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1e45d1c5-247f-43fd-9f4a-dcf08740536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "input_size = 8  # Size of input noise vector\n",
    "noise_dim = input_size\n",
    "metadata_size = 5  # Size of metadata array\n",
    "metadata_dim = metadata_size\n",
    "output_size = dataset[0][0].shape[0]  # Size of output audio vector\n",
    "num_epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f5708fb5-9d99-41ef-ae0f-d9ff6c5d1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "generator = GeneratorDTW(input_size, metadata_size, output_size).to(device)\n",
    "discriminator = DiscriminatorDTW(output_size).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "01566897-beeb-4a5e-ac33-e9cf0a10f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = nn.BCELoss()  # Binary cross-entropy loss for real/fake classification\\\n",
    "g_adversarial_loss = DynamicTimeWarpingLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()  # Cross-entropy loss for classifying the generated number\n",
    "\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "60c8c3ec-df28-47f3-a91e-d2bab0b69c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_gan_dtw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[142], line 66\u001b[0m, in \u001b[0;36mtrain_gan_dtw\u001b[1;34m(generator, discriminator, data_loader, num_epochs, noise_dim, metadata_dim, device)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# g_loss_classification = classification_loss(generated_number, labels.float())\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# g_loss_total = g_loss + g_loss_classification\u001b[39;00m\n\u001b[0;32m     65\u001b[0m g_loss_total \u001b[38;5;241m=\u001b[39m g_loss\n\u001b[1;32m---> 66\u001b[0m \u001b[43mg_loss_total\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Print training progress\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "train_gan_dtw(generator, discriminator, dataloader, num_epochs, noise_dim, metadata_dim, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d46def-decb-47b1-9074-9cf80b0b860c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e20f843-14d3-43f9-90c9-1e6c378fefe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae738e7-6b49-4130-9236-c39b20639aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
