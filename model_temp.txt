class TransformerEncoderModel(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward, num_layers):
        super(TransformerEncoderModel, self).__init__()
        self.encoder_layer = TransformerEncoderLayer(d_model=d_model,
                                                     nhead=nhead,
                                                     dim_feedforward=dim_feedforward)
        self.encoder = TransformerEncoder(self.encoder_layer, num_layers=num_layers)

    def forward(self, inputs):
        x = self.encoder(inputs)
        return x


class ConvExtractor(nn.Module):
    def __init__(self, input_dims, hidden_dims, out_dims, kernel_size):
        super(ConvExtractor, self).__init__()
        self.input_dims = input_dims
        self.hidden_dims = hidden_dims
        self.out_dims = out_dims
        self.kernel_size = kernel_size
        self.conv1 = nn.Conv1d(in_channels=self.input_dims, out_channels=self.hidden_dims, kernel_size=self.kernel_size)
        self.norm1 = nn.BatchNorm1d(num_features=self.hidden_dims)
        self.conv2 = nn.Conv1d(in_channels=self.hidden_dims, out_channels=self.hidden_dims,
                               kernel_size=self.kernel_size)
        self.activation1 = nn.SiLU()
        self.conv3 = nn.Conv1d(in_channels=self.hidden_dims, out_channels=self.out_dims, kernel_size=self.kernel_size)
        self.activation2 = nn.SiLU()
        self.activation3 = nn.SiLU()

    def forward(self, inputs):
        x = inputs.type(torch.float32)
        x = self.conv1(x)
        # x = self.norm1(x)
        x = self.activation1(x)
        x = self.conv2(x)
        x = self.activation2(x)
        x = self.conv3(x)
        x = self.activation3(x)
        return x

class LSTM(nn.Module):
    def __init__(self, num_classes, input_size, hidden_size, num_layers):
        super(LSTM, self).__init__()

        self.num_classes = num_classes
        self.num_layers = num_layers
        self.input_size = input_size
        self.hidden_size = hidden_size

        self.lstm1 = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size,
                             num_layers=self.num_layers, batch_first=True)
        self.norm1 = nn.BatchNorm1d(num_features=self.hidden_size)
        self.activation1 = nn.SiLU()
        self.lstm2 = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size,
                             num_layers=self.num_layers, batch_first=True)
        self.activation2 = nn.SiLU()
        self.lstm3 = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size,
                             num_layers=self.num_layers, batch_first=True)
        self.activation3 = nn.SiLU()
        self.fc = nn.Linear(self.hidden_size, self.num_classes)

    def forward(self, x):
        h_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        c_0 = Variable(torch.zeros(
            self.num_layers, x.size(0), self.hidden_size))

        ula1, (h_out1, c_out1) = self.lstm1(x, (h_0, c_0))
        ula1 = self.activation1(ula1)
        ula2, (h_out2, c_out2) = self.lstm2(ula1, (h_out1, c_out1))
        ula2 = self.activation2(ula2)
        # ula3, (h_out3, c_out_3) = self.lstm2(ula2, (h_out2, c_out2))  # added later
        # ula3 = self.activation2(ula3)  # added later
        # enc = ula3.view(-1, self.hidden_size)  # added later
        enc = ula2.view(-1, self.hidden_size)
        out = self.fc(enc)
        return out


class MLP(nn.Module):
    def __init__(self,
                 input_dims,
                 hidden_dims,
                 output_dims):
        super(MLP, self).__init__()
        self.lin1 = nn.Linear(in_features=input_dims, out_features=hidden_dims)
        self.norm1 = nn.BatchNorm1d(num_features=hidden_dims)
        self.activation1 = nn.ReLU()
        self.lin2 = nn.Linear(in_features=hidden_dims, out_features=hidden_dims)
        self.norm2 = nn.BatchNorm1d(num_features=hidden_dims)
        self.activation2 = nn.ReLU()
        self.lin3 = nn.Linear(in_features=hidden_dims, out_features=hidden_dims)
        self.activation3 = nn.ReLU()
        self.lin4 = nn.Linear(in_features=hidden_dims, out_features=output_dims)

    def forward(self, inputs):
        x = inputs
        x = self.lin1(x)
        x = self.activation1(x)
        x = self.lin2(x)
        x = self.activation2(x)
        # x = self.lin3(x)  # added later
        # x = self.activation3(x)  # added later
        x = self.lin4(x)
        return x

class CTLModel(nn.Module):
    def __init__(self,
                 d_model_transformer,
                 nhead_transformer,
                 dim_feedforward_transformer,
                 num_layers_transformer,
                 input_dims_extractor,
                 hidden_dims_extractor,
                 out_dims_extractor,
                 kernel_size_extractor,
                 num_classes_lstm,
                 input_size_lstm,
                 hidden_size_lstm,
                 num_layers_lstm,
                 in_dims_mlp,
                 hidden_dims_mlp,
                 out_dims_mlp):
        super(CTLModel, self).__init__()
        self.encoder = TransformerEncoderModel(d_model=d_model_transformer,
                                               nhead=nhead_transformer,
                                               dim_feedforward=dim_feedforward_transformer,
                                               num_layers=num_layers_transformer)
        self.ext = ConvExtractor(input_dims=input_dims_extractor,
                                 hidden_dims=hidden_dims_extractor,
                                 out_dims=out_dims_extractor,
                                 kernel_size=kernel_size_extractor)
        self.lstm = LSTM(num_classes=num_classes_lstm,
                         input_size=input_size_lstm,
                         hidden_size=hidden_size_lstm,
                         num_layers=num_layers_lstm)
        self.mlp = MLP(input_dims=in_dims_mlp,
                       hidden_dims=hidden_dims_mlp,
                       output_dims=out_dims_mlp)

    def forward(self, x_trunk, x_branch):
        x_branch = x_branch.type(torch.float32)
        out = self.encoder(x_branch).unsqueeze(1)
        out2 = self.ext(out)
        out3 = out2.squeeze(1)
        x_trunk = x_trunk.type(torch.float32).unsqueeze(1)
        out4 = self.lstm(x_trunk)
        output = torch.bmm(out3.unsqueeze(1), out4.unsqueeze(2)).squeeze(1)
        return output
