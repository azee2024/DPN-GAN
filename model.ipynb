{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9250f7c6-1a7f-4f72-a8bd-8d576f9ea486",
   "metadata": {},
   "source": [
    "### **Formating the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f0a24cc4-0108-4dfa-971a-eea1b09ff446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import librosa\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2e6b01de-2d6d-40b6-8428-be3edb13c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "605c5ee0-6761-41f8-9d4d-9054d1ab4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioMNISTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.file_list = os.listdir(self.data_path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_name = os.path.join(self.data_path, self.file_list[idx])\n",
    "        data = np.load(file_name)\n",
    "        \n",
    "        # print(self.file_list[idx])\n",
    "        label, subfolder_index, data_index = self.file_list[idx].split(\"_\")\n",
    "        \n",
    "        # Extracting data\n",
    "        metadata = data[\"metadata\"]\n",
    "        audio = data[\"audio\"]\n",
    "        mel_spec = data[\"mel_spec\"]\n",
    "        mel_spec_db = data[\"mel_spec_db\"]\n",
    "\n",
    "        # Assuming your data is a numpy array, you can convert it to a PyTorch tensor\n",
    "        tensor_audio = torch.from_numpy(audio).reshape(-1, 1)\n",
    "        tensor_meta = torch.tensor(metadata).reshape(-1, 1)\n",
    "        tensor_label = torch.nn.functional.one_hot(torch.tensor(int(label)), num_classes=10).reshape(-1, 1)\n",
    "        tensor_mel_spec = torch.tensor(mel_spec)\n",
    "        tensor_mel_spec_db = torch.tensor(mel_spec_db)\n",
    "        return tensor_audio, tensor_meta, tensor_label, tensor_mel_spec, tensor_mel_spec_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2414dd54-d88e-456e-9cfb-0bf56b95624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioMNISTDataset(data_path=\"./AudioMNIST/preprocessed_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6380392c-7fa0-40a2-b15a-dfef584ea7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for batching and shuffling\n",
    "batch_size = 32\n",
    "shuffle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2c625c6a-c19f-4de5-8304-c82917873fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b2331-aae7-470f-a7c9-d82f07862414",
   "metadata": {},
   "source": [
    "## **Model Development**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1712e5-b6cf-4ee2-980f-0667a80a3d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2954ffed-6157-4894-ba3a-309c7c7db2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d26633d-34d5-44e8-bc30-6bc485871c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a87e3c-b7a2-418d-8efd-af6ed6c27402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4a7fd-f854-40e6-8491-7005a3cc9701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afec2bd-4fd5-4780-b718-d809d964dd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1ae3d0-febe-4be6-a0c7-c70bca0ec7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd4f05-133e-452a-ac58-888c7da3e1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cbd39-8bb8-446b-b602-afe8e51ed056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30dc2b-9087-440a-b446-8b01afafdab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510b34f-f048-4a6c-acad-bf541ec77f58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354091c-aebf-4916-9502-5b8640e15cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b6419-d8a4-4ec0-9189-384c260d878f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
